['gpt2-small(124M)_768_12_128.csv', 'gpt2-large(774M)_1280_20_256.csv', 'gpt2-small(124M)_768_12_1024.csv', 'gpt2-xl(1558M)_1600_25_128.csv', 'gpt2-xl(1558M)_1600_25_1024.csv', 'gpt2-medium(355M)_1024_16_128.csv', 'gpt2-small(124M)_768_12_2048.csv', 'gpt2-xl(1558M)_1600_25_256.csv', 'gpt2-small(124M)_768_12_512.csv', 'gpt2-xl(1558M)_1600_25_512.csv', 'gpt2-small(124M)_768_12_256.csv', 'gpt2-medium(355M)_1024_16_2048.csv', 'gpt2-medium(355M)_1024_16_512.csv', 'gpt2-xl(1558M)_1600_25_2048.csv', 'gpt2-small(124M)_768_12_4096.csv', 'gpt2-medium(355M)_1024_16_256.csv', 'gpt2-xl(1558M)_1600_25_4096.csv', 'gpt2-medium(355M)_1024_16_4096.csv', 'gpt2-large(774M)_1280_20_2048.csv', 'gpt2-large(774M)_1280_20_1024.csv', 'gpt2-large(774M)_1280_20_4096.csv', 'gpt2-medium(355M)_1024_16_1024.csv', 'gpt2-large(774M)_1280_20_512.csv', 'gpt2-large(774M)_1280_20_128.csv']
====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-small(124M)_768_12_128.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  64
ofmap_sram_write 128
Compute cycles: 766
Stall cycles: 0
Memory Access: 192
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 0.977 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  128
ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Memory Access: 192
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.031 words/cycle
Average Filter DRAM BW: 0.977 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 18384
Total Compute Cycles: 59796
Attention %: 0.3074453140678306
Total Attention Memory Access: 4608
Total Memory Access: 32256
Attention Memory Access %: 0.14285714285714285
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 128
Configuration: gpt2-small(124M)_768_12_128


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-large(774M)_1280_20_256.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  6400
ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Memory Access: 12800
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.305 words/cycle
Average Filter DRAM BW: 9.766 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
ifmap_sram_reads  6400
ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Memory Access: 12800
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.305 words/cycle
Average Filter DRAM BW: 9.766 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
ifmap_sram_reads  6400
ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Memory Access: 12800
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.305 words/cycle
Average Filter DRAM BW: 9.766 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  64
ofmap_sram_write 256
Compute cycles: 766
Stall cycles: 0
Memory Access: 320
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 1.953 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  256
ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Memory Access: 320
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.061 words/cycle
Average Filter DRAM BW: 1.953 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  6400
ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Memory Access: 12800
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.305 words/cycle
Average Filter DRAM BW: 9.766 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  6400
ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Memory Access: 12800
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.305 words/cycle
Average Filter DRAM BW: 9.766 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  6400
ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Memory Access: 12800
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.305 words/cycle
Average Filter DRAM BW: 9.766 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 30640
Total Compute Cycles: 145684
Attention %: 0.21031822300321243
Total Attention Memory Access: 12800
Total Memory Access: 89600
Attention Memory Access %: 0.14285714285714285
Model Name and Parameters gpt2-large(774M)
Embeddding Dimension: 1280
Number of Heads: 20
Context Length 256
Configuration: gpt2-large(774M)_1280_20_256


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-small(124M)_768_12_1024.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  256
ofmap_sram_write 1024
Compute cycles: 3067
Stall cycles: 0
Memory Access: 1280
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 7.812 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  1024
ofmap_sram_write 256
Compute cycles: 3067
Stall cycles: 0
Memory Access: 1280
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 7.812 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 73608
Total Compute Cycles: 115020
Attention %: 0.6399582681272822
Total Attention Memory Access: 30720
Total Memory Access: 58368
Attention Memory Access %: 0.5263157894736842
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 1024
Configuration: gpt2-small(124M)_768_12_1024


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-xl(1558M)_1600_25_128.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  64
ofmap_sram_write 128
Compute cycles: 766
Stall cycles: 0
Memory Access: 192
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 0.977 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  128
ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Memory Access: 192
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.031 words/cycle
Average Filter DRAM BW: 0.977 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 38300
Total Compute Cycles: 263792
Attention %: 0.14519014981500578
Total Attention Memory Access: 9600
Total Memory Access: 144000
Attention Memory Access %: 0.06666666666666667
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 128
Configuration: gpt2-xl(1558M)_1600_25_128


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-xl(1558M)_1600_25_1024.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  256
ofmap_sram_write 1024
Compute cycles: 3067
Stall cycles: 0
Memory Access: 1280
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 7.812 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  1024
ofmap_sram_write 256
Compute cycles: 3067
Stall cycles: 0
Memory Access: 1280
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 7.812 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 153350
Total Compute Cycles: 378842
Attention %: 0.40478616415286583
Total Attention Memory Access: 64000
Total Memory Access: 198400
Attention Memory Access %: 0.3225806451612903
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 1024
Configuration: gpt2-xl(1558M)_1600_25_1024


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-medium(355M)_1024_16_128.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  64
ofmap_sram_write 128
Compute cycles: 766
Stall cycles: 0
Memory Access: 192
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 0.977 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  128
ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Memory Access: 192
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.031 words/cycle
Average Filter DRAM BW: 0.977 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 24512
Total Compute Cycles: 98138
Attention %: 0.24977073101143288
Total Attention Memory Access: 6144
Total Memory Access: 55296
Attention Memory Access %: 0.1111111111111111
Model Name and Parameters gpt2-medium(355M)
Embeddding Dimension: 1024
Number of Heads: 16
Context Length 128
Configuration: gpt2-medium(355M)_1024_16_128


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-small(124M)_768_12_2048.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  512
ofmap_sram_write 2048
Compute cycles: 6135
Stall cycles: 0
Memory Access: 2560
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 7.812 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  2048
ofmap_sram_write 512
Compute cycles: 6135
Stall cycles: 0
Memory Access: 2560
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.488 words/cycle
Average Filter DRAM BW: 7.812 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 147240
Total Compute Cycles: 188652
Attention %: 0.7804847019909675
Total Attention Memory Access: 61440
Total Memory Access: 89088
Attention Memory Access %: 0.6896551724137931
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 2048
Configuration: gpt2-small(124M)_768_12_2048


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-xl(1558M)_1600_25_256.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  64
ofmap_sram_write 256
Compute cycles: 766
Stall cycles: 0
Memory Access: 320
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 1.953 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  256
ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Memory Access: 320
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.061 words/cycle
Average Filter DRAM BW: 1.953 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 38300
Total Compute Cycles: 263792
Attention %: 0.14519014981500578
Total Attention Memory Access: 16000
Total Memory Access: 150400
Attention Memory Access %: 0.10638297872340426
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 256
Configuration: gpt2-xl(1558M)_1600_25_256


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-small(124M)_768_12_512.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  128
ofmap_sram_write 512
Compute cycles: 1533
Stall cycles: 0
Memory Access: 640
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 3.906 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  512
ofmap_sram_write 128
Compute cycles: 1533
Stall cycles: 0
Memory Access: 640
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 3.906 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 36792
Total Compute Cycles: 78204
Attention %: 0.47046186895810954
Total Attention Memory Access: 15360
Total Memory Access: 43008
Attention Memory Access %: 0.35714285714285715
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 512
Configuration: gpt2-small(124M)_768_12_512


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-xl(1558M)_1600_25_512.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  128
ofmap_sram_write 512
Compute cycles: 1533
Stall cycles: 0
Memory Access: 640
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 3.906 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  512
ofmap_sram_write 128
Compute cycles: 1533
Stall cycles: 0
Memory Access: 640
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 3.906 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 76650
Total Compute Cycles: 302142
Attention %: 0.25368866294656156
Total Attention Memory Access: 32000
Total Memory Access: 166400
Attention Memory Access %: 0.19230769230769232
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 512
Configuration: gpt2-xl(1558M)_1600_25_512


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-small(124M)_768_12_256.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  64
ofmap_sram_write 256
Compute cycles: 766
Stall cycles: 0
Memory Access: 320
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 1.953 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  256
ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Memory Access: 320
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.061 words/cycle
Average Filter DRAM BW: 1.953 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  2304
ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Memory Access: 4608
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.183 words/cycle
Average Filter DRAM BW: 8.789 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 18384
Total Compute Cycles: 59796
Attention %: 0.3074453140678306
Total Attention Memory Access: 7680
Total Memory Access: 35328
Attention Memory Access %: 0.21739130434782608
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 256
Configuration: gpt2-small(124M)_768_12_256


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-medium(355M)_1024_16_2048.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  512
ofmap_sram_write 2048
Compute cycles: 6135
Stall cycles: 0
Memory Access: 2560
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 7.812 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  2048
ofmap_sram_write 512
Compute cycles: 6135
Stall cycles: 0
Memory Access: 2560
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.488 words/cycle
Average Filter DRAM BW: 7.812 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 196320
Total Compute Cycles: 269946
Attention %: 0.7272565624235958
Total Attention Memory Access: 81920
Total Memory Access: 131072
Attention Memory Access %: 0.625
Model Name and Parameters gpt2-medium(355M)
Embeddding Dimension: 1024
Number of Heads: 16
Context Length 2048
Configuration: gpt2-medium(355M)_1024_16_2048


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-medium(355M)_1024_16_512.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
ifmap_sram_reads  128
ofmap_sram_write 512
Compute cycles: 1533
Stall cycles: 0
Memory Access: 640
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.015 words/cycle
Average Filter DRAM BW: 3.906 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
ifmap_sram_reads  512
ofmap_sram_write 128
Compute cycles: 1533
Stall cycles: 0
Memory Access: 640
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 3.906 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 5 : Add_norm
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
ifmap_sram_reads  4096
ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Memory Access: 8192
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.244 words/cycle
Average Filter DRAM BW: 9.615 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 49056
Total Compute Cycles: 122682
Attention %: 0.3998630605956864
Total Attention Memory Access: 20480
Total Memory Access: 69632
Attention Memory Access %: 0.29411764705882354
Model Name and Parameters gpt2-medium(355M)
Embeddding Dimension: 1024
Number of Heads: 16
Context Length 512
Configuration: gpt2-medium(355M)_1024_16_512


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	4096
SRAM Filter (kB): 	8192
SRAM OFMAP (kB): 	4096
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/gpt_models/gpt2-xl(1558M)_1600_25_2048.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
ifmap_sram_reads  11200
ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Memory Access: 22400
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.381 words/cycle
Average Filter DRAM BW: 9.844 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
