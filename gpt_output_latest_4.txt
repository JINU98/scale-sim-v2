====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-small(124M)_768_12_128.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  64
self.ofmap_sram_write 128
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 1.302 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  128
self.ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.020 words/cycle
Average Filter DRAM BW: 1.302 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 18384
Total Compute Cycles: 61328
Attention %: 0.2997651969736499
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 128
Configuration: gpt2-small(124M)_768_12_128


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-large(774M)_1280_20_256.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  64
self.ofmap_sram_write 256
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 2.604 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  256
self.ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.041 words/cycle
Average Filter DRAM BW: 2.604 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 30640
Total Compute Cycles: 147216
Attention %: 0.2081295511357461
Model Name and Parameters gpt2-large(774M)
Embeddding Dimension: 1280
Number of Heads: 20
Context Length 256
Configuration: gpt2-large(774M)_1280_20_256


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-small(124M)_768_12_1024.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  256
self.ofmap_sram_write 1024
Compute cycles: 3067
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  1024
self.ofmap_sram_write 256
Compute cycles: 3067
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 73608
Total Compute Cycles: 121154
Attention %: 0.6075573237367318
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 1024
Configuration: gpt2-small(124M)_768_12_1024


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-xl(1558M)_1600_25_128.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  64
self.ofmap_sram_write 128
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 1.302 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  128
self.ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.020 words/cycle
Average Filter DRAM BW: 1.302 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 38300
Total Compute Cycles: 265324
Attention %: 0.14435181137024922
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 128
Configuration: gpt2-xl(1558M)_1600_25_128


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-xl(1558M)_1600_25_1024.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  256
self.ofmap_sram_write 1024
Compute cycles: 3067
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  1024
self.ofmap_sram_write 256
Compute cycles: 3067
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 153350
Total Compute Cycles: 384976
Attention %: 0.39833651967914885
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 1024
Configuration: gpt2-xl(1558M)_1600_25_1024


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-medium(355M)_1024_16_128.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  64
self.ofmap_sram_write 128
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 1.302 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  128
self.ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.020 words/cycle
Average Filter DRAM BW: 1.302 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 24512
Total Compute Cycles: 99670
Attention %: 0.245931574194843
Model Name and Parameters gpt2-medium(355M)
Embeddding Dimension: 1024
Number of Heads: 16
Context Length 128
Configuration: gpt2-medium(355M)_1024_16_128


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-small(124M)_768_12_2048.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  512
self.ofmap_sram_write 2048
Compute cycles: 6135
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 6.944 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  2048
self.ofmap_sram_write 512
Compute cycles: 6135
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.325 words/cycle
Average Filter DRAM BW: 6.944 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 147240
Total Compute Cycles: 200922
Attention %: 0.7328216919998806
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 2048
Configuration: gpt2-small(124M)_768_12_2048


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-xl(1558M)_1600_25_256.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  64
self.ofmap_sram_write 256
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 2.604 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  256
self.ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.041 words/cycle
Average Filter DRAM BW: 2.604 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 38300
Total Compute Cycles: 265324
Attention %: 0.14435181137024922
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 256
Configuration: gpt2-xl(1558M)_1600_25_256


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-small(124M)_768_12_512.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  128
self.ofmap_sram_write 512
Compute cycles: 1533
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  512
self.ofmap_sram_write 128
Compute cycles: 1533
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.081 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 36792
Total Compute Cycles: 81270
Attention %: 0.45271317829457364
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 512
Configuration: gpt2-small(124M)_768_12_512


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-xl(1558M)_1600_25_512.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  128
self.ofmap_sram_write 512
Compute cycles: 1533
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  512
self.ofmap_sram_write 128
Compute cycles: 1533
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.081 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 76650
Total Compute Cycles: 305208
Attention %: 0.2511402060234332
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 512
Configuration: gpt2-xl(1558M)_1600_25_512


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-small(124M)_768_12_256.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  64
self.ofmap_sram_write 256
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 2.604 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  256
self.ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.041 words/cycle
Average Filter DRAM BW: 2.604 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 18384
Total Compute Cycles: 61328
Attention %: 0.2997651969736499
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 256
Configuration: gpt2-small(124M)_768_12_256


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-medium(355M)_1024_16_2048.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  512
self.ofmap_sram_write 2048
Compute cycles: 6135
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 6.944 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  2048
self.ofmap_sram_write 512
Compute cycles: 6135
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.325 words/cycle
Average Filter DRAM BW: 6.944 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 196320
Total Compute Cycles: 282216
Attention %: 0.6956373841313037
Model Name and Parameters gpt2-medium(355M)
Embeddding Dimension: 1024
Number of Heads: 16
Context Length 2048
Configuration: gpt2-medium(355M)_1024_16_2048


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-medium(355M)_1024_16_512.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  128
self.ofmap_sram_write 512
Compute cycles: 1533
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  512
self.ofmap_sram_write 128
Compute cycles: 1533
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.081 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 49056
Total Compute Cycles: 125748
Attention %: 0.39011356045424184
Model Name and Parameters gpt2-medium(355M)
Embeddding Dimension: 1024
Number of Heads: 16
Context Length 512
Configuration: gpt2-medium(355M)_1024_16_512


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-xl(1558M)_1600_25_2048.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  512
self.ofmap_sram_write 2048
Compute cycles: 6135
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 6.944 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  2048
self.ofmap_sram_write 512
Compute cycles: 6135
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.325 words/cycle
Average Filter DRAM BW: 6.944 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 306750
Total Compute Cycles: 544512
Attention %: 0.5633484661495064
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 2048
Configuration: gpt2-xl(1558M)_1600_25_2048


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-small(124M)_768_12_4096.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  1024
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 8.333 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  4096
self.ofmap_sram_write 1024
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.651 words/cycle
Average Filter DRAM BW: 8.333 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  2304
self.ofmap_sram_write 2304
Compute cycles: 6902
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.122 words/cycle
Average Filter DRAM BW: 9.375 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 294504
Total Compute Cycles: 360458
Attention %: 0.8170272264729871
Model Name and Parameters gpt2-small(124M)
Embeddding Dimension: 768
Number of Heads: 12
Context Length 4096
Configuration: gpt2-small(124M)_768_12_4096


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-medium(355M)_1024_16_256.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  64
self.ofmap_sram_write 256
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 2.604 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  256
self.ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.041 words/cycle
Average Filter DRAM BW: 2.604 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 24512
Total Compute Cycles: 99670
Attention %: 0.245931574194843
Model Name and Parameters gpt2-medium(355M)
Embeddding Dimension: 1024
Number of Heads: 16
Context Length 256
Configuration: gpt2-medium(355M)_1024_16_256


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-xl(1558M)_1600_25_4096.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  1024
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 8.333 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  4096
self.ofmap_sram_write 1024
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.651 words/cycle
Average Filter DRAM BW: 8.333 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  11200
self.ofmap_sram_write 11200
Compute cycles: 37582
Stall cycles: 0
Overall utilization: 0.10%
Mapping efficiency: 79.72%
Average IFMAP DRAM BW: 0.254 words/cycle
Average Filter DRAM BW: 9.924 words/cycle
Average OFMAP DRAM BW: 254.545 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 613550
Total Compute Cycles: 863584
Attention %: 0.7104693926705451
Model Name and Parameters gpt2-xl(1558M)
Embeddding Dimension: 1600
Number of Heads: 25
Context Length 4096
Configuration: gpt2-xl(1558M)_1600_25_4096


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-medium(355M)_1024_16_4096.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  1024
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 8.333 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  4096
self.ofmap_sram_write 1024
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.651 words/cycle
Average Filter DRAM BW: 8.333 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 392672
Total Compute Cycles: 490840
Attention %: 0.8
Model Name and Parameters gpt2-medium(355M)
Embeddding Dimension: 1024
Number of Heads: 16
Context Length 4096
Configuration: gpt2-medium(355M)_1024_16_4096


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-large(774M)_1280_20_2048.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  512
self.ofmap_sram_write 2048
Compute cycles: 6135
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 6.944 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  2048
self.ofmap_sram_write 512
Compute cycles: 6135
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.325 words/cycle
Average Filter DRAM BW: 6.944 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 245400
Total Compute Cycles: 372714
Attention %: 0.658413689853346
Model Name and Parameters gpt2-large(774M)
Embeddding Dimension: 1280
Number of Heads: 20
Context Length 2048
Configuration: gpt2-large(774M)_1280_20_2048


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-large(774M)_1280_20_1024.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  256
self.ofmap_sram_write 1024
Compute cycles: 3067
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  1024
self.ofmap_sram_write 256
Compute cycles: 3067
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 122680
Total Compute Cycles: 243858
Attention %: 0.5030796611142551
Model Name and Parameters gpt2-large(774M)
Embeddding Dimension: 1280
Number of Heads: 20
Context Length 1024
Configuration: gpt2-large(774M)_1280_20_1024


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-large(774M)_1280_20_4096.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  1024
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 8.333 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  4096
self.ofmap_sram_write 1024
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.651 words/cycle
Average Filter DRAM BW: 8.333 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 490840
Total Compute Cycles: 630426
Attention %: 0.7785846395929102
Model Name and Parameters gpt2-large(774M)
Embeddding Dimension: 1280
Number of Heads: 20
Context Length 4096
Configuration: gpt2-large(774M)_1280_20_4096


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-medium(355M)_1024_16_1024.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  256
self.ofmap_sram_write 1024
Compute cycles: 3067
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  1024
self.ofmap_sram_write 256
Compute cycles: 3067
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  4096
self.ofmap_sram_write 4096
Compute cycles: 12271
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.163 words/cycle
Average Filter DRAM BW: 9.804 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 98144
Total Compute Cycles: 177904
Attention %: 0.5516683154959978
Model Name and Parameters gpt2-medium(355M)
Embeddding Dimension: 1024
Number of Heads: 16
Context Length 1024
Configuration: gpt2-medium(355M)_1024_16_1024


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-large(774M)_1280_20_512.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  128
self.ofmap_sram_write 512
Compute cycles: 1533
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  512
self.ofmap_sram_write 128
Compute cycles: 1533
Stall cycles: 0
Overall utilization: 0.03%
Mapping efficiency: 25.00%
Average IFMAP DRAM BW: 0.081 words/cycle
Average Filter DRAM BW: 5.208 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 61320
Total Compute Cycles: 179430
Attention %: 0.34174887142618293
Model Name and Parameters gpt2-large(774M)
Embeddding Dimension: 1280
Number of Heads: 20
Context Length 512
Configuration: gpt2-large(774M)_1280_20_512


************ SCALE SIM Run Complete ****************


====================================================
******************* SCALE SIM **********************
====================================================
Array Size: 	256x256
SRAM IFMAP (kB): 	6144
SRAM Filter (kB): 	6144
SRAM OFMAP (kB): 	2048
Dataflow: 	Weight Stationary
CSV file path: 	/home/jmalekar/Documents/scale-sim-v2/topologies/decoder_models_new/gpt2-large(774M)_1280_20_128.csv
Working in ESTIMATE BANDWIDTH mode.
====================================================

Running Layer 0 : Q
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 1 : K
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 2 : V
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 3 : Attention_score
self.ifmap_sram_reads  64
self.ofmap_sram_write 128
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.010 words/cycle
Average Filter DRAM BW: 1.302 words/cycle
Average OFMAP DRAM BW: 128.000 words/cycle

Running Layer 4 : Attention_output
self.ifmap_sram_reads  128
self.ofmap_sram_write 64
Compute cycles: 766
Stall cycles: 0
Overall utilization: 0.02%
Mapping efficiency: 12.50%
Average IFMAP DRAM BW: 0.020 words/cycle
Average Filter DRAM BW: 1.302 words/cycle
Average OFMAP DRAM BW: 64.000 words/cycle

Running Layer 5 : Add_norm
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 6 : Dense_in
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle

Running Layer 7 : Dense_out
self.ifmap_sram_reads  6400
self.ofmap_sram_write 6400
Compute cycles: 19174
Stall cycles: 0
Overall utilization: 0.13%
Mapping efficiency: 100.00%
Average IFMAP DRAM BW: 0.203 words/cycle
Average Filter DRAM BW: 9.645 words/cycle
Average OFMAP DRAM BW: 256.000 words/cycle


----------------------------------------------------------------------------------------


Total Attention Cycles: 30640
Total Compute Cycles: 147216
Attention %: 0.2081295511357461
Model Name and Parameters gpt2-large(774M)
Embeddding Dimension: 1280
Number of Heads: 20
Context Length 128
Configuration: gpt2-large(774M)_1280_20_128


************ SCALE SIM Run Complete ****************


